execution_prompt:
  id: "RRNET_EXEC_INTEGRATION_03_PERFORMANCE_BENCHMARKING"
  purpose: >
    Implement comprehensive performance benchmarking system for RRNet SaaS
    including load testing, stress testing, performance monitoring,
    bottleneck detection, and performance analytics across all modules
    with automated benchmark execution and performance regression detection.

  prerequisites:
    - "EXECUTION_INTEGRATION_02 Cross-Module Validation completed"
    - "All backend modules completed (01-10.1)"
    - "All super admin modules completed (SA_01-SA_06)"
    - "Performance monitoring infrastructure available"

  design_principles:
    - "Performance benchmarks cover all critical workflows"
    - "Load testing validates system under expected load"
    - "Stress testing identifies system breaking points"
    - "Performance monitoring provides real-time insights"
    - "Performance regression testing prevents degradation"
    - "Benchmark results drive optimization decisions"

  scope:
    include:
      - load_testing
      - stress_testing
      - performance_monitoring
      - bottleneck_detection
      - performance_analytics
      - regression_testing
      - benchmark_automation
      - performance_reporting
    exclude:
      - unit_performance_tests
      - frontend_performance_tests
      - network_performance_tests
      - database_performance_tuning

  backend_structure_additions:
    internal/performance:
      - benchmarking/
      - monitoring/
      - analytics/
      - regression/
    internal/performance/benchmarking:
      - load_tester.go
      - stress_tester.go
      - workflow_benchmarks.go
      - module_benchmarks.go
    internal/performance/monitoring:
      - metrics_collector.go
      - performance_monitor.go
      - bottleneck_detector.go
    internal/performance/analytics:
      - performance_analyzer.go
      - trend_analyzer.go
      - report_generator.go
    internal/performance/regression:
      - regression_detector.go
      - baseline_manager.go
      - comparison_engine.go

  performance_categories:
  load_testing:
  concurrent_user_simulation:
    description: "Test system performance under expected concurrent user load"
    scenarios:
      - 100_concurrent_users
      - 500_concurrent_users
      - 1000_concurrent_users
      - 2000_concurrent_users
    workflows:
      - user_authentication
      - client_management
      - invoice_generation
      - payment_processing
      - dashboard_access
    metrics:
      - response_time_p95
      - response_time_p99
      - throughput_requests_per_second
      - error_rate
      - cpu_usage
      - memory_usage
      - database_connections

  api_endpoint_load_testing:
    description: "Test individual API endpoints under load"
    critical_endpoints:
      - authentication_endpoints
      - tenant_management_endpoints
      - client_crud_endpoints
      - billing_endpoints
      - network_management_endpoints
      - collector_endpoints
      - maps_endpoints
      - super_admin_endpoints
    load_patterns:
      - steady_load
      - ramp_up_load
      - burst_load
    metrics:
      - endpoint_response_time
      - endpoint_throughput
      - endpoint_error_rate
      - resource_utilization

  database_load_testing:
    description: "Test database performance under load"
    operations:
      - read_operations
      - write_operations
      - mixed_operations
      - complex_queries
      - transaction_operations
    metrics:
      - query_response_time
      - transaction_time
      - connection_pool_usage
      - database_cpu_usage
      - database_memory_usage
      - disk_io_usage

  stress_testing:
  system_limit_testing:
    description: "Test system limits and breaking points"
    stress_scenarios:
      - maximum_concurrent_users
      - maximum_data_volume
      - maximum_api_requests
      - maximum_background_jobs
      - maximum_database_connections
    failure_points:
      - memory_exhaustion
      - cpu_exhaustion
      - database_connection_exhaustion
      - disk_space_exhaustion
      - network_bandwidth_exhaustion
    metrics:
      - breaking_point_load
      - failure_recovery_time
      - data_corruption_check
      - system_stability_check

  resource_exhaustion_testing:
    description: "Test system behavior under resource exhaustion"
    exhaustion_scenarios:
      - memory_pressure
      - cpu_pressure
      - disk_io_pressure
      - network_pressure
      - database_pressure
    recovery_testing:
      - graceful_degradation
      - automatic_recovery
      - manual_recovery
      - data_integrity_check
    metrics:
      - degradation_performance
      - recovery_time
      - data_loss_check
      - system_stability_check

  performance_monitoring:
  real_time_monitoring:
    description: "Real-time performance monitoring across all modules"
    monitoring_points:
      - api_response_time
      - database_query_time
      - background_job_processing_time
      - cache_hit_rate
      - memory_usage
      - cpu_usage
      - disk_io
      - network_io
    collection_frequency:
      - real_time_metrics: 10_seconds
      - aggregated_metrics: 1_minute
      - historical_metrics: 5_minutes
    alerting:
      - response_time_thresholds
      - error_rate_thresholds
      - resource_usage_thresholds
      - custom_thresholds

  performance_metrics:
    application_metrics:
      - request_duration_histogram
      - request_rate_counter
      - error_rate_counter
      - active_connections_gauge
      - queue_depth_gauge
    system_metrics:
      - cpu_usage_percent
      - memory_usage_percent
      - disk_usage_percent
      - network_io_bytes
      - disk_io_bytes
    business_metrics:
      - user_registrations_rate
      - invoice_generation_rate
      - payment_processing_rate
      - collector_completion_rate

  bottleneck_detection:
  performance_bottlenecks:
    description: "Detect and analyze performance bottlenecks"
    detection_methods:
      - slow_query_detection
      - api_endpoint_analysis
      - background_job_analysis
      - resource_usage_analysis
      - dependency_analysis
    bottleneck_types:
      - database_bottlenecks
      - application_bottlenecks
      - network_bottlenecks
      - resource_bottlenecks
      - external_service_bottlenecks
    analysis_tools:
      - query_profiler
      - application_profiler
      - system_profiler
      - network_analyzer

  optimization_recommendations:
    description: "Generate performance optimization recommendations"
    recommendation_categories:
      - database_optimizations
      - application_optimizations
      - infrastructure_optimizations
      - architecture_optimizations
    recommendation_engine:
      - performance_pattern_matching
      - best_practices_analysis
      - cost_benefit_analysis
      - implementation_priority

  performance_analytics:
  trend_analysis:
    description: "Analyze performance trends over time"
    trend_metrics:
      - response_time_trends
      - throughput_trends
      - error_rate_trends
      - resource_usage_trends
    analysis_periods:
      - hourly_trends
      - daily_trends
      - weekly_trends
      - monthly_trends
    trend_detection:
      - performance_degradation
      - performance_improvement
      - seasonal_variations
      - anomaly_detection

  comparative_analysis:
    description: "Compare performance across different scenarios"
    comparison_types:
      - before_after_deployment
      - different_load_patterns
      - different_configurations
      - different_environments
    comparison_metrics:
      - performance_improvement_percentage
      - regression_detection
      - configuration_impact
      - environment_differences

  performance_reporting:
  benchmark_reports:
    description: "Generate comprehensive performance benchmark reports"
    report_types:
      - load_test_report
      - stress_test_report
      - performance_summary_report
      - trend_analysis_report
      - bottleneck_report
    report_contents:
      - executive_summary
      - detailed_metrics
      - performance_charts
      - recommendations
      - action_items

  dashboard_visualization:
    description: "Create performance monitoring dashboards"
    dashboard_types:
      - real_time_performance_dashboard
      - historical_trends_dashboard
      - bottleneck_analysis_dashboard
      - system_health_dashboard
    visualization_elements:
      - performance_charts
      - metric_gauges
      - trend_graphs
      - alert_indicators

  regression_testing:
  performance_regression:
    description: "Detect performance regressions across deployments"
    regression_detection:
      - baseline_comparison
      - threshold_based_detection
      - statistical_analysis
      - automated_regression_alerts
    regression_types:
      - response_time_regression
      - throughput_regression
      - error_rate_regression
      - resource_usage_regression
    regression_analysis:
      - regression_impact_assessment
      - root_cause_analysis
      - rollback_recommendations

  baseline_management:
    description: "Manage performance baselines for regression testing"
    baseline_types:
      - performance_baselines
      - load_baselines
      - stress_baselines
      - resource_baselines
    baseline_management:
      - baseline_creation
      - baseline_updates
      - baseline_versioning
      - baseline_comparison

  benchmark_automation:
  automated_benchmark_execution:
    description: "Automated benchmark execution system"
    execution_triggers:
      - scheduled_benchmarks
      - pre_deployment_benchmarks
      - post_deployment_benchmarks
      - on_demand_benchmarks
    automation_features:
      - test_environment_setup
      - benchmark_execution
      - result_collection
      - report_generation
      - alert_notification

  continuous_integration:
    description: "Integrate performance testing into CI/CD pipeline"
    integration_points:
      - pre_commit_performance_checks
      - pull_request_performance_tests
      - deployment_gate_performance_tests
      - post_deployment_performance_validation
    ci_integration_features:
      - automated_test_execution
      - performance_gate_enforcement
      - regression_detection
      - performance_trend_tracking

  performance_environments:
  test_environments:
    description: "Dedicated performance testing environments"
    environment_types:
      - load_testing_environment
      - stress_testing_environment
      - benchmarking_environment
      - regression_testing_environment
    environment_requirements:
      - production_like_configuration
      - realistic_data_volumes
      - monitoring_integration
      - result_collection

  data_generation:
    description: "Generate realistic test data for performance testing"
    data_types:
      - tenant_data
      - user_data
      - client_data
      - billing_data
      - network_data
    data_generation_features:
      - realistic_data_volumes
      - data_relationships
      - data_variations
      - data_consistency

  coding_rules:
    - "Benchmarks must be reproducible"
    - "Benchmarks must be automated"
    - "Benchmarks must have clear success criteria"
    - "Benchmarks must provide actionable insights"
    - "Benchmarks must be versioned"
    - "Benchmarks must be integrated with CI/CD"

  output_expectations:
    - "Comprehensive performance benchmarking system"
    - "Automated load and stress testing"
    - "Real-time performance monitoring"
    - "Bottleneck detection and analysis"
    - "Performance analytics and reporting"
    - "Performance regression detection"
    - "Benchmark automation and CI/CD integration"

  stop_condition:
    - "Load testing functional for all critical workflows"
    - "Stress testing identifies system limits"
    - "Performance monitoring provides real-time insights"
    - "Bottleneck detection working effectively"
    - "Performance analytics providing actionable insights"
    - "Performance regression detection functional"
    - "Benchmark automation integrated with CI/CD"
    - "Ready for security audit"

  instruction_to_ai:
    - "Implement comprehensive performance benchmarking"
    - "Focus on critical business workflows"
    - "Ensure benchmarks are automated and repeatable"
    - "Provide clear performance insights and recommendations"
    - "Do not implement unit performance tests"
